{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hildafs/projects/phy230056p/junzhez/AI/maskgit/gmmg\n",
      "batchnorm.ipynb  layers.ipynb  \u001b[0m\u001b[38;5;33msrc\u001b[0m/         tf_datasets.ipynb\n",
      "\u001b[38;5;33mconfig\u001b[0m/          nets.ipynb    test1.ipynb  train_token.ipynb\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import time\n",
    "from absl import logging\n",
    "from typing import Any\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "%cd /hildafs/projects/phy230056p/junzhez/AI/maskgit/gmmg\n",
    "%ls\n",
    "\n",
    "# Import custom modules\n",
    "from src.layer import get_norm_layer, tensorflow_style_avg_pooling, upsample, dsample\n",
    "from src.loss import squared_euclidean_distance, entropy_loss, l1_loss, l2_loss\n",
    "from src.vqvae import VQVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (we do not need label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save images\n",
    "data_dir = '/hildafs/projects/phy230056p/junzhez/data/'\n",
    "data_name = 'mnist'\n",
    "save_dir = os.path.join(data_dir, data_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load MNIST dataset using TensorFlow Datasets\n",
    "dataset_name = 'mnist'\n",
    "train_split = 'train[:80%]'\n",
    "val_split = 'train[80%:]'\n",
    "test_split = 'test'\n",
    "\n",
    "train_dataset, train_info = tfds.load(dataset_name, data_dir=save_dir, split=train_split, with_info=True)\n",
    "val_dataset = tfds.load(dataset_name, data_dir=save_dir, split=val_split)\n",
    "test_dataset = tfds.load(dataset_name, data_dir=save_dir, split=test_split)\n",
    "\n",
    "# Preprocess dataset\n",
    "def preprocess(sample):\n",
    "    image = sample['image']\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return {'image': image}\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess)\n",
    "val_dataset = val_dataset.map(preprocess)\n",
    "test_dataset = test_dataset.map(preprocess)\n",
    "\n",
    "# Convert to numpy arrays and batch the data\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 18:26:40.450653: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-05-29 18:26:40.496198: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-05-29 18:26:40.532586: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape = (batch, height, width, channels)\n",
      "Train data shape: (32, 28, 28, 1)\n",
      "Val data shape: (32, 28, 28, 1)\n",
      "Test data shape: (32, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example training, validation, and test data\n",
    "train_example = next(iter(train_dataset.as_numpy_iterator()))\n",
    "val_example = next(iter(val_dataset.as_numpy_iterator()))\n",
    "test_example = next(iter(test_dataset.as_numpy_iterator()))\n",
    "\n",
    "print('shape = (batch, height, width, channels)')\n",
    "print(f\"Train data shape: {train_example['image'].shape}\")\n",
    "print(f\"Val data shape: {val_example['image'].shape}\")\n",
    "print(f\"Test data shape: {test_example['image'].shape}\")\n",
    "\n",
    "# plot example data\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(train_example['image'][0])\n",
    "axs[1].imshow(val_example['image'][0])\n",
    "axs[2].imshow(test_example['image'][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VQVAE model configuration\n",
    "config = ConfigDict()\n",
    "config.vqvae = ConfigDict()\n",
    "\n",
    "# encoder configs\n",
    "config.vqvae.filters = 64\n",
    "config.vqvae.num_res_blocks = 2\n",
    "config.vqvae.channel_multipliers = [1, 1, 2]\n",
    "config.vqvae.embedding_dim = 32 # output dimension of the encoder\n",
    "config.vqvae.conv_downsample = False # whether to use convolutional downsampling or average pooling downsampling in the encoder/decoder\n",
    "config.vqvae.conv_fn = \"conv\" # the standard convolutional layer in the residual block, and in the encoder (flax.linen.Conv)\n",
    "config.vqvae.norm_type = \"GN\" # normalization layer in the residual block\n",
    "config.vqvae.activation_fn = \"swish\" # activation function in the residual block, and in the encoder\n",
    "\n",
    "# additional decoder configs\n",
    "config.vqvae.output_dim = 1 # N of decoder output channels = N color channels \n",
    "\n",
    "\n",
    "config.vqvae.codebook_size = 512\n",
    "config.vqvae.commitment_cost = 0.25\n",
    "\n",
    "config.vqvae.entropy_loss_ratio = 0.0\n",
    "config.vqvae.entropy_loss_type = \"softmax\"\n",
    "config.vqvae.entropy_temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.vqvae = ml_collections.ConfigDict()\n",
    "# config.vqvae.quantizer = \"vq\"\n",
    "config.vqvae.codebook_size = 1024\n",
    "\n",
    "config.vqvae.entropy_loss_ratio = 0.1\n",
    "config.vqvae.entropy_temperature = 0.01\n",
    "config.vqvae.entropy_loss_type = \"softmax\"\n",
    "config.vqvae.commitment_cost = 0.25\n",
    "\n",
    "config.vqvae.filters = 128\n",
    "config.vqvae.num_res_blocks = 2\n",
    "config.vqvae.channel_multipliers = [1, 1, 2, 2, 4]\n",
    "config.vqvae.embedding_dim = 256\n",
    "config.vqvae.conv_downsample = False\n",
    "config.vqvae.activation_fn = \"swish\"\n",
    "config.vqvae.norm_type = \"GN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VQVAE model\n",
    "vqvae = VQVAE(config=config, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 02:50:51.788571: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['params', 'batch_stats'])\n",
      "dict_keys(['decoder', 'encoder', 'quantizer'])\n",
      "dict_keys(['decoder', 'encoder'])\n",
      "dict_keys(['BatchNorm_0', 'Conv_0', 'Conv_1', 'Conv_2', 'Conv_3', 'ResBlock_0', 'ResBlock_1', 'ResBlock_2', 'ResBlock_3', 'ResBlock_4', 'ResBlock_5', 'ResBlock_6', 'ResBlock_7'])\n",
      "dict_keys(['BatchNorm_0', 'Conv_0', 'Conv_1', 'ResBlock_0', 'ResBlock_1', 'ResBlock_2', 'ResBlock_3', 'ResBlock_4', 'ResBlock_5', 'ResBlock_6', 'ResBlock_7'])\n",
      "dict_keys(['codebook'])\n",
      "dict_keys(['BatchNorm_0', 'ResBlock_0', 'ResBlock_1', 'ResBlock_2', 'ResBlock_3', 'ResBlock_4', 'ResBlock_5', 'ResBlock_6', 'ResBlock_7'])\n",
      "dict_keys(['BatchNorm_0', 'ResBlock_0', 'ResBlock_1', 'ResBlock_2', 'ResBlock_3', 'ResBlock_4', 'ResBlock_5', 'ResBlock_6', 'ResBlock_7'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 02:50:52.039960: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "ModifyScopeVariableError",
     "evalue": "Cannot update variable \"mean\" in \"/encoder/ResBlock_0/BatchNorm_0\" because collection \"batch_stats\" is immutable. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ModifyScopeVariableError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModifyScopeVariableError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    105\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m--> 107\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 83\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config, train_dataset, val_dataset, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     81\u001b[0m val_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_dataset\u001b[38;5;241m.\u001b[39mas_numpy_iterator():\n\u001b[0;32m---> 83\u001b[0m     loss, recon_loss, quantizer_loss \u001b[38;5;241m=\u001b[39m \u001b[43meval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     85\u001b[0m     val_recon_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m recon_loss\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[15], line 58\u001b[0m, in \u001b[0;36meval_step\u001b[0;34m(state, batch)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(variables[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_stats\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;66;03m# dict_keys(['conv1', 'res_block_0', 'res_block_1', 'conv2'])\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# access train in variables\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m reconstructions, _ \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m l2_loss(batch, reconstructions)\n\u001b[1;32m     60\u001b[0m quantizer_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Assuming quantizer_loss is not required during evaluation\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m/hildafs/projects/phy230056p/junzhez/AI/maskgit/gmmg/src/tokenizer.py:306\u001b[0m, in \u001b[0;36mVQVAE.__call__\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dict):\n\u001b[0;32m--> 306\u001b[0m     quantized, result_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(quantized)\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs, result_dict\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/hildafs/projects/phy230056p/junzhez/AI/maskgit/gmmg/src/tokenizer.py:275\u001b[0m, in \u001b[0;36mVQVAE.encode\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dict):\n\u001b[1;32m    274\u001b[0m     image \u001b[38;5;241m=\u001b[39m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 275\u001b[0m     encoded_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     quantized, result_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantizer(encoded_feature)\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m quantized, result_dict\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/hildafs/projects/phy230056p/junzhez/AI/maskgit/gmmg/src/tokenizer.py:109\u001b[0m, in \u001b[0;36mEncoder.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m filters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_multipliers[i]\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_res_blocks):\n\u001b[0;32m--> 109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mResBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mblock_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m num_blocks \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_downsample:\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/hildafs/projects/phy230056p/junzhez/AI/maskgit/gmmg/src/tokenizer.py:45\u001b[0m, in \u001b[0;36mResBlock.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     44\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(x)\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)(x)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/hildafs/projects/phy230056p/junzhez/.conda/envs/AI_torch/lib/python3.10/site-packages/flax/linen/normalization.py:369\u001b[0m, in \u001b[0;36mBatchNorm.__call__\u001b[0;34m(self, x, use_running_average, mask)\u001b[0m\n\u001b[1;32m    357\u001b[0m   mean, var \u001b[38;5;241m=\u001b[39m _compute_stats(\n\u001b[1;32m    358\u001b[0m       x,\n\u001b[1;32m    359\u001b[0m       reduction_axes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m       force_float32_reductions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_float32_reductions,\n\u001b[1;32m    366\u001b[0m   )\n\u001b[1;32m    368\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_initializing():\n\u001b[0;32m--> 369\u001b[0m     ra_mean\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    370\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;241m*\u001b[39m ra_mean\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum) \u001b[38;5;241m*\u001b[39m mean\n\u001b[1;32m    371\u001b[0m     )\n\u001b[1;32m    372\u001b[0m     ra_var\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;241m*\u001b[39m ra_var\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum) \u001b[38;5;241m*\u001b[39m var\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _normalize(\n\u001b[1;32m    375\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    376\u001b[0m   x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_init,\n\u001b[1;32m    388\u001b[0m )\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/hildafs/projects/phy230056p/junzhez/.conda/envs/AI_torch/lib/python3.10/site-packages/flax/core/scope.py:798\u001b[0m, in \u001b[0;36mScope.put_variable\u001b[0;34m(self, col, name, value)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_trace_level()\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(col):\n\u001b[0;32m--> 798\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mModifyScopeVariableError(col, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[1;32m    799\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutable_collection(col)\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Make sure reference sharing of child variable dictionaries isn't broken.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# See https://github.com/google/flax/issues/2022 for more details.\u001b[39;00m\n",
      "\u001b[0;31mModifyScopeVariableError\u001b[0m: Cannot update variable \"mean\" in \"/encoder/ResBlock_0/BatchNorm_0\" because collection \"batch_stats\" is immutable. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ModifyScopeVariableError)"
     ]
    }
   ],
   "source": [
    "# Define a simple training state to hold the model and optimizer\n",
    "class TrainState(train_state.TrainState):\n",
    "    batch_stats: Any = None\n",
    "\n",
    "def create_train_state(rng, config, learning_rate):\n",
    "    vqvae = VQVAE(config=config, train=True)\n",
    "    variables = vqvae.init(rng, {'image': jnp.ones([1, 28, 28, 1]), 'train': True})\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=vqvae.apply, params=params, tx=tx, batch_stats=batch_stats)\n",
    "\n",
    "def l2_loss(y_true, y_pred):\n",
    "    # y_true has shape (batch_size, height, width, color_channels=1)\n",
    "    diff = y_true - y_pred\n",
    "    diff = jnp.asarray(diff, jnp.float32)\n",
    "    return jnp.mean(jnp.square(diff))\n",
    "\n",
    "# Training step\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        variables = {'params': params, 'batch_stats': state.batch_stats}\n",
    "        result, new_model_state = state.apply_fn(variables, {'image': batch, 'train': True}, mutable=['batch_stats'])\n",
    "        reconstructions, result_dict = result\n",
    "        # result_dict has keys: \n",
    "        # dict_keys(['quantizer_loss', 'e_latent_loss', 'q_latent_loss', 'entropy_loss', 'encodings', 'encoding_indices', 'raw'])\n",
    "        recon_loss = l2_loss(batch, reconstructions)\n",
    "        quantizer_loss = result_dict['quantizer_loss']\n",
    "        total_loss = recon_loss + quantizer_loss\n",
    "        return total_loss, (recon_loss, quantizer_loss, new_model_state)\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (total_loss, (recon_loss, quantizer_loss, new_model_state)), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    state = state.replace(batch_stats=new_model_state['batch_stats'])\n",
    "    return state, total_loss, recon_loss, quantizer_loss\n",
    "\n",
    "\n",
    "\n",
    "# Validation step\n",
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    variables = {'params': state.params, 'batch_stats': state.batch_stats}\n",
    "    print(variables.keys()) # dict_keys(['params', 'batch_stats'])\n",
    "\n",
    "    print(variables['params'].keys()) # dict_keys(['decoder', 'encoder', 'quantizer'])\n",
    "    print(variables['params']['decoder'].keys()) # dict_keys(['BatchNorm_0', 'Conv_0', 'Conv_1', 'Conv_2', 'Conv_3', 'ResBlock_0', 'ResBlock_1', 'ResBlock_2', 'ResBlock_3', 'ResBlock_4', 'ResBlock_5', 'ResBlock_6', 'ResBlock_7'])\n",
    "    print(variables['params']['encoder'].keys()) # dict_keys(['BatchNorm_0', 'Conv_0', 'Conv_1', 'ResBlock_0', 'ResBlock_1', 'ResBlock_2', 'ResBlock_3', 'ResBlock_4', 'ResBlock_5', 'ResBlock_6', 'ResBlock_7'])\n",
    "    print(variables['params']['quantizer'].keys()) # dict_keys(['codebook'])\n",
    "\n",
    "    print(variables['batch_stats'].keys()) # dict_keys(['decoder', 'encoder'])\n",
    "    print(variables['batch_stats']['decoder'].keys()) # dict_keys(['BatchNorm_0', 'ResBlock_0', 'ResBlock_1', 'ResBlock_2', 'ResBlock_3', 'ResBlock_4', 'ResBlock_5', 'ResBlock_6', 'ResBlock_7'])\n",
    "    print(variables['batch_stats']['encoder'].keys()) # dict_keys(['BatchNorm_0', 'ResBlock_0', 'ResBlock_1', 'ResBlock_2', 'ResBlock_3', 'ResBlock_4', 'ResBlock_5', 'ResBlock_6', 'ResBlock_7'])\n",
    "\n",
    "    # access train in variables\n",
    "    reconstructions, _ = state.apply_fn(variables, {'image': batch, 'train': False}, mutable=False)\n",
    "    recon_loss = l2_loss(batch, reconstructions)\n",
    "    quantizer_loss = 0  # Assuming quantizer_loss is not required during evaluation\n",
    "    total_loss = recon_loss + quantizer_loss\n",
    "    return total_loss, recon_loss, quantizer_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(config, train_dataset, val_dataset, num_epochs, learning_rate):\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    state = create_train_state(rng, config, learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        for batch in train_dataset.as_numpy_iterator():\n",
    "            state, total_loss, recon_loss, quantizer_loss = train_step(state, batch)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        val_recon_loss = 0\n",
    "        val_quantizer_loss = 0\n",
    "        val_steps = 0\n",
    "        for batch in val_dataset.as_numpy_iterator():\n",
    "            loss, recon_loss, quantizer_loss = eval_step(state, batch)\n",
    "            val_loss += loss\n",
    "            val_recon_loss += recon_loss\n",
    "            val_quantizer_loss += quantizer_loss\n",
    "            val_steps += 1\n",
    "        val_loss /= val_steps\n",
    "        val_recon_loss /= val_steps\n",
    "        val_quantizer_loss /= val_steps\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}, Time: {epoch_time:.2f}s, \"\n",
    "            f\"Train Loss: {total_loss:.4f}, \"\n",
    "            f\"Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"Recon Loss: {val_recon_loss:.4f}, \"\n",
    "            f\"Quantizer Loss: {val_quantizer_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# Start training\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_and_evaluate(config, train_dataset, val_dataset, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "state = create_train_state(rng, config, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['step', 'apply_fn', 'params', 'tx', 'opt_state', 'batch_stats'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder', 'quantizer', 'decoder'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 01:55:55.130233: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     89\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m---> 91\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 60\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config, train_dataset, val_dataset, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataset\u001b[38;5;241m.\u001b[39mas_numpy_iterator():\n\u001b[0;32m---> 60\u001b[0m     state, total_loss, recon_loss, quantizer_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m     63\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(state, batch)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss, (recon_loss, quantizer_loss), new_model_state\n\u001b[1;32m     32\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss_fn, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 33\u001b[0m (total_loss, (recon_loss, quantizer_loss), new_model_state), grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m new_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads, batch_stats\u001b[38;5;241m=\u001b[39mnew_model_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_stats\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_state, total_loss, recon_loss, quantizer_loss\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36mtrain_step.<locals>.loss_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     24\u001b[0m variables \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_stats\u001b[39m\u001b[38;5;124m'\u001b[39m: state\u001b[38;5;241m.\u001b[39mbatch_stats}\n\u001b[1;32m     25\u001b[0m outputs, new_model_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_fn(\n\u001b[1;32m     26\u001b[0m     variables, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: batch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}, mutable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_stats\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m reconstructions, quantizer_loss \u001b[38;5;241m=\u001b[39m outputs\n\u001b[1;32m     29\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m l2_loss(batch, reconstructions)\n\u001b[1;32m     30\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m quantizer_loss\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Define a simple training state to hold the model and optimizer\n",
    "class TrainState(train_state.TrainState):\n",
    "    batch_stats: Any = None\n",
    "\n",
    "def create_train_state(rng, config, learning_rate):\n",
    "    vqvae = VQVAE(config=config, train=True)\n",
    "    variables = vqvae.init(rng, {'image': jnp.ones([1, 28, 28, 1]), 'train': True})\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=vqvae.apply, params=params, tx=tx, batch_stats=batch_stats)\n",
    "\n",
    "\n",
    "def l2_loss(y_true, y_pred):\n",
    "    diff = y_true - y_pred\n",
    "    diff = jnp.asarray(diff, jnp.float32)\n",
    "    return jnp.mean(jnp.square(diff))\n",
    "\n",
    "\n",
    "# Training step\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        variables = {'params': params, 'batch_stats': state.batch_stats}\n",
    "        outputs, new_model_state = state.apply_fn(\n",
    "            variables, {'image': batch, 'train': True}, mutable=['batch_stats']\n",
    "        )\n",
    "        reconstructions, quantizer_loss = outputs\n",
    "        recon_loss = l2_loss(batch, reconstructions)\n",
    "        total_loss = recon_loss + quantizer_loss\n",
    "        return total_loss, (recon_loss, quantizer_loss), new_model_state\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (total_loss, (recon_loss, quantizer_loss), new_model_state), grads = grad_fn(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads, batch_stats=new_model_state['batch_stats'])\n",
    "    return new_state, total_loss, recon_loss, quantizer_loss\n",
    "\n",
    "\n",
    "# Validation step\n",
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    variables = {'params': state.params, 'batch_stats': state.batch_stats}\n",
    "    outputs = state.apply_fn(\n",
    "        variables, {'image': batch, 'train': False}, mutable=False\n",
    "    )\n",
    "    reconstructions, quantizer_loss = outputs\n",
    "    recon_loss = l2_loss(batch, reconstructions)\n",
    "    total_loss = recon_loss + quantizer_loss\n",
    "    return total_loss, recon_loss, quantizer_loss\n",
    "\n",
    "\n",
    "def train_and_evaluate(config, train_dataset, val_dataset, num_epochs, learning_rate):\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    state = create_train_state(rng, config, learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        for batch in train_dataset.as_numpy_iterator():\n",
    "            state, total_loss, recon_loss, quantizer_loss = train_step(state, batch)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        val_recon_loss = 0\n",
    "        val_quantizer_loss = 0\n",
    "        val_steps = 0\n",
    "        for batch in val_dataset.as_numpy_iterator():\n",
    "            loss, recon_loss, quantizer_loss = eval_step(state, batch)\n",
    "            val_loss += loss\n",
    "            val_recon_loss += recon_loss\n",
    "            val_quantizer_loss += quantizer_loss\n",
    "            val_steps += 1\n",
    "        val_loss /= val_steps\n",
    "        val_recon_loss /= val_steps\n",
    "        val_quantizer_loss /= val_steps\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        logging.info(\n",
    "            f\"Epoch {epoch+1}, Time: {epoch_time:.2f}s, \"\n",
    "            f\"Train Loss: {total_loss:.4f}, \"\n",
    "            f\"Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"Recon Loss: {val_recon_loss:.4f}, \"\n",
    "            f\"Quantizer Loss: {val_quantizer_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Start training\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_and_evaluate(config, train_dataset, val_dataset, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
