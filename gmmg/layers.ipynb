{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hildafs/projects/phy230056p/junzhez/AI/maskgit\n",
      "\u001b[0m\u001b[38;5;33mcheckpoints\u001b[0m/     \u001b[38;5;33mgmmg\u001b[0m/  LICENSE   MaskGIT_demo.ipynb  requirements.txt\n",
      "CONTRIBUTING.md  \u001b[38;5;33mimgs\u001b[0m/  \u001b[38;5;33mmaskgit\u001b[0m/  README.md\n",
      "[cuda(id=0)]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import Any\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import ml_collections\n",
    "\n",
    "%cd /hildafs/projects/phy230056p/junzhez/AI/maskgit\n",
    "%ls\n",
    "import maskgit\n",
    "from maskgit.nets import layers\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "key = random.PRNGKey(42)\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _l2_normalize function\n",
    "- each row of x is normalized to have unit L2 norm\n",
    "$$\n",
    "\\texttt{l2normalize}(x) = \\frac{x}{\\sqrt{\\sum_{i=1}^{n} x_i^2 + \\epsilon}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array:\n",
      "[[ 0.6122652   1.1225883   1.1373317 ]\n",
      " [-0.8127325  -0.890405    0.12623145]]\n",
      "L2-normalized array:\n",
      "[[ 0.35777485  0.65598017  0.6645954 ]\n",
      " [-0.6704925  -0.73457116  0.1041391 ]] [0.99999994 0.99999994]\n"
     ]
    }
   ],
   "source": [
    "# Test _l2_normalize function: each row of x is normalized to have unit L2 norm\n",
    "x = random.normal(key, (2, 3))\n",
    "print(\"Original array:\")\n",
    "print(x)\n",
    "normalized_x = layers._l2_normalize(x, axis=-1)\n",
    "print(\"L2-normalized array:\")\n",
    "print(normalized_x, jnp.linalg.norm(normalized_x, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_norm_layer function\n",
    "- returns a normalization layer, depending on the `norm_type` specified\n",
    "- Batch Normalization (BN)\n",
    "$$\n",
    "y = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n",
    "$$\n",
    "- Layer Normalization (LN)\n",
    "$$\n",
    "y = \\frac{x - \\text{mean}(x)}{\\sqrt{\\text{var}(x) + \\epsilon}} \\cdot \\gamma + \\beta\n",
    "$$\n",
    "- Group Normalization (GN)\n",
    "$$\n",
    "y = \\frac{x - \\text{mean}_{\\text{group}}(x)}{\\sqrt{\\text{var}_{\\text{group}}(x) + \\epsilon}} \\cdot \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "- The input tensor `x` typically has shape `(B, H, W, C)` or `(B, C, H, W)`, where `B` is the batch size, `H` is the height, `W` is the width, and `C` is the number of channels.\n",
    "- $\\mu$ and $\\sigma$ are the mean and standard deviation of the input tensor `x`, e.g., across the batch dimension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape\n",
      "(2, 4, 4, 3) (shape: batch_size x height x width x channels)\n",
      "BatchNorm(\n",
      "    # attributes\n",
      "    use_running_average = False\n",
      "    axis = -1\n",
      "    momentum = 0.9\n",
      "    epsilon = 1e-05\n",
      "    dtype = float32\n",
      "    param_dtype = float32\n",
      "    use_bias = True\n",
      "    use_scale = True\n",
      "    bias_init = zeros\n",
      "    scale_init = ones\n",
      "    axis_name = None\n",
      "    axis_index_groups = None\n",
      "    use_fast_variance = True\n",
      "    force_float32_reductions = True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor\n",
    "x = random.normal(key, (2, 4, 4, 3))  # Example shape: (batch_size, height, width, channels)\n",
    "print(\"Input tensor shape\")\n",
    "print(x.shape, '(shape: batch_size x height x width x channels)')\n",
    "\n",
    "norm_layer_fn = layers.get_norm_layer(train=True, dtype=jnp.float32, norm_type='BN')\n",
    "norm_layer = norm_layer_fn()\n",
    "print(norm_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow_style_avg_pooling function\n",
    "- This function performs average pooling in a manner similar to TensorFlow, excluding padding cells from the average calculation.\n",
    "- Pooling is a layer in convolutional neural networks (CNNs) that reduces the spatial dimensions of input data while keeping the most important information. Pooling layers are also known as downsample layers.\n",
    "- **Mathematical Expression:**\n",
    "$$\n",
    "y_{i,j} = \\frac{1}{|P_{i,j}|} \\sum_{(m,n) \\in P_{i,j}} x_{m,n}\n",
    "$$\n",
    "where $P_{i,j}$ is the pooling window centered at $(i,j)$, and $|P_{i,j}|$ is the number of elements in $P_{i,j}$.\n",
    "\n",
    "**In this context, more specifically:**\n",
    "- Given:\n",
    "  - Input tensor `x` with shape `(N, H, W, C)`\n",
    "  - Pooling window shape `(hw, ww)`\n",
    "  - Strides `(hs, ws)`. Physically, this means the window moves `hs` steps in the height dimension and `ws` steps in the width dimension.\n",
    "  - Padding mode: either \"SAME\" or \"VALID\"\n",
    "- The function does two main operations:\n",
    "    1. **Sum the elements within each pooling window**: This is done using `jax.lax.reduce_window` with `jax.lax.add`.\n",
    "    2. **Count the number of elements within each pooling window**: This is also done using `jax.lax.reduce_window` with `jax.lax.add` on a tensor of ones.\n",
    "- **Formula Steps:**\n",
    "    1. **Pooling Window Sum (pool_sum)**:\n",
    "$$\n",
    "\\text{pool-sum}_{i, j, k, l} = \\sum_{m=0}^{h_w-1} \\sum_{n=0}^{w_w-1} x_{i, (j \\cdot h_s + m), (k \\cdot w_s + n), l}\n",
    "$$\n",
    "where $i$ indexes the batch, $j$ and $k$ index the spatial dimensions, and $l$ indexes the channels. This sum considers the elements within the pooling window.\n",
    "    2. **Pooling Window Denominator (pool_denom)**:\n",
    "$$\n",
    "\\text{pool-denom}_{i, j, k, l} = \\sum_{m=0}^{h_w-1} \\sum_{n=0}^{w_w-1} 1\n",
    "$$\n",
    "which simply counts the number of elements within each pooling window, excluding any padding cells.\n",
    "    3. **Average Pooling Calculation**:\n",
    "$$\n",
    "y_{i, j, k, l} = \\frac{\\text{pool-sum}_{i, j, k, l}}{\\text{pool-denom}_{i, j, k, l}}\n",
    "$$\n",
    "Here, $y$ is the output tensor after average pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:, dimensions: batch_size x height x width x channels\n",
      "(1, 4, 4, 2)\n",
      "Pooled tensor: dimensions: batch_size x height x width x channels\n",
      "(1, 2, 2, 2)\n",
      "first channel of x\n",
      "[[ 0  2  4  6]\n",
      " [ 8 10 12 14]\n",
      " [16 18 20 22]\n",
      " [24 26 28 30]]\n",
      "first channel of pooled_x\n",
      "[[ 5.  9.]\n",
      " [21. 25.]]\n"
     ]
    }
   ],
   "source": [
    "# Test tensorflow_style_avg_pooling function\n",
    "x = random.normal(key, (1, 4, 4, 2)) \n",
    "x = jnp.arange(32).reshape(1, 4, 4, 2)\n",
    "pooled_x = layers.tensorflow_style_avg_pooling(x, window_shape=(2, 2), strides=(2, 2), padding='SAME')\n",
    "print(\"Original tensor:, dimensions: batch_size x height x width x channels\")\n",
    "print(x.shape)\n",
    "print(\"Pooled tensor:\", \"dimensions: batch_size x height x width x channels\")\n",
    "print(pooled_x.shape)\n",
    "print('first channel of x')\n",
    "print(x[0, :, :, 0])\n",
    "print('first channel of pooled_x')\n",
    "print(pooled_x[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample and Downsample\n",
    "- Upsample: Increase the spatial dimensions of the input tensor\n",
    "$$\n",
    "y = \\text{resize}(x, (n, h \\cdot \\text{factor}, w \\cdot \\text{factor}, c))\n",
    "$$\n",
    "- Downsample: Decrease the spatial dimensions of the input tensor\n",
    "$$\n",
    "y_{i,j} = \\frac{1}{4} \\sum_{(m,n) \\in P_{i,j}} x_{m,n}\n",
    "$$\n",
    "- i.e., tensorflow_style_avg_pooling with `hw=2, ww=2, hs=2, ws=2`, i.e., window size is 2x2, stride is 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:, dimensions: batch_size x height x width x channels\n",
      "(1, 4, 4, 2)\n",
      "Upsampled tensor: dimensions: batch_size x height x width x channels\n",
      "(1, 8, 8, 2)\n",
      "Downsampled tensor: dimensions: batch_size x height x width x channels\n",
      "(1, 2, 2, 2)\n",
      "first channel of x\n",
      "[[ 0  2  4  6]\n",
      " [ 8 10 12 14]\n",
      " [16 18 20 22]\n",
      " [24 26 28 30]]\n",
      "first channel of upsampled_x\n",
      "[[ 0  0  2  2  4  4  6  6]\n",
      " [ 0  0  2  2  4  4  6  6]\n",
      " [ 8  8 10 10 12 12 14 14]\n",
      " [ 8  8 10 10 12 12 14 14]\n",
      " [16 16 18 18 20 20 22 22]\n",
      " [16 16 18 18 20 20 22 22]\n",
      " [24 24 26 26 28 28 30 30]\n",
      " [24 24 26 26 28 28 30 30]]\n",
      "first channel of downsampled_x\n",
      "[[ 5.  9.]\n",
      " [21. 25.]]\n"
     ]
    }
   ],
   "source": [
    "# Test upsample function\n",
    "x = jnp.arange(32).reshape(1, 4, 4, 2)\n",
    "print(\"Original tensor:, dimensions: batch_size x height x width x channels\")\n",
    "print(x.shape)\n",
    "\n",
    "upsampled_x = layers.upsample(x, factor=2)\n",
    "print(\"Upsampled tensor:\", \"dimensions: batch_size x height x width x channels\")\n",
    "print(upsampled_x.shape)\n",
    "\n",
    "downsampled_x = layers.dsample(x)\n",
    "print(\"Downsampled tensor:\", \"dimensions: batch_size x height x width x channels\")\n",
    "print(downsampled_x.shape)\n",
    "\n",
    "print('first channel of x')\n",
    "print(x[0, :, :, 0])\n",
    "print('first channel of upsampled_x')\n",
    "print(upsampled_x[0, :, :, 0])\n",
    "print('first channel of downsampled_x')\n",
    "print(downsampled_x[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
